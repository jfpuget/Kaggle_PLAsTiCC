{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'lgb_1181'\n",
    "fname_base = 'base_006'\n",
    "fname_bazin = 'bazin_003'\n",
    "fname_newling = 'newling_003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of train time augmentations.\n",
    "n_tta = 6\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import curve_fit\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "from tqdm import tqdm_notebook\n",
    "import itertools\n",
    "import pickle as pkl\n",
    "\n",
    "pd.options.display.max_columns = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "def init_seeds(seed):\n",
    "\n",
    "    # The below is necessary for starting Numpy generated random numbers\n",
    "    # in a well-defined initial state.\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # The below is necessary for starting core Python generated random numbers\n",
    "    # in a well-defined state.\n",
    "\n",
    "    rn.seed(seed)\n",
    "\n",
    "\n",
    "init_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger():\n",
    "    logger_ = logging.getLogger('main')\n",
    "    logger_.setLevel(logging.DEBUG)\n",
    "    fh = logging.FileHandler('simple_lightgbm.log')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    # add the handlers to the logger\n",
    "    logger_.addHandler(fh)\n",
    "    logger_.addHandler(ch)\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    return logging.getLogger('main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_multi_weighted_logloss(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'xgb_multi_weighted_loss', lgb_multi_weighted_logloss(labels, preds)\n",
    "\n",
    "def eval_lgb_multi_weighted_logloss(preds, train_data, n_tta=n_tta):\n",
    "    \n",
    "    label = train_data.get_label()\n",
    "    classes = list(range(14))\n",
    "    class_weight = {0: 1, 1: 2, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 2, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1}\n",
    "    res = lgb_multi_weighted_logloss(label, preds, classes, class_weight, n_tta)\n",
    "    \n",
    "    return res\n",
    "    \n",
    "def lgb_multi_weighted_logloss(y_true, y_preds,\n",
    "                               classes=[6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95],\n",
    "                               class_weight={6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, \n",
    "                                             65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1},\n",
    "                               n_tta = n_tta,\n",
    "):\n",
    "    \n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_preds = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "    \n",
    "    size = y_true.shape[0] // n_tta\n",
    "    y_true = y_true[:size]\n",
    "    y_p = np.zeros((size, len(classes)))\n",
    "    \n",
    "    for i in range(n_tta):\n",
    "         y_p += y_preds[i * size : (i+1) * size]\n",
    "    y_p /= n_tta\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "\n",
    "def multi_weighted_logloss(y_true, y_preds,\n",
    "                              classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95],\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, \n",
    "                    64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    ):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    plt.figure(figsize=(8, 12))\n",
    "    sns.barplot(x='gain', y='feature', data=importances_.sort_values('mean_gain', ascending=False))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importances(clfs):\n",
    "    importances = [clf.feature_importance('gain') for clf in clfs]\n",
    "    importances = np.vstack(importances)\n",
    "    mean_gain = np.mean(importances, axis=0)\n",
    "    features = clfs[0].feature_name()\n",
    "    data = pd.DataFrame({'gain':mean_gain, 'feature':features})\n",
    "    plt.figure(figsize=(8, 30))\n",
    "    sns.barplot(x='gain', y='feature', data=data.sort_values('gain', ascending=False))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('importances.png')\n",
    "    return data\n",
    "\n",
    "def train_classifiers(lgb_params, full_train=None, y=None, w=None, verbose=2000, \n",
    "                      folds=5, ttas=None):\n",
    "    print(full_train.shape[1], 'features')\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    tta_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in tqdm_notebook(enumerate(kf.split(y, y)), total=folds):\n",
    "        print()\n",
    "        print('fold %2d' % fold_)\n",
    "        trn_x, trn_y, trn_w = full_train.iloc[trn_], y.iloc[trn_], w.iloc[trn_]\n",
    "        val_x, val_y, val_w = full_train.iloc[val_], y.iloc[val_], w.iloc[val_]\n",
    "        size = val_y.shape[0]\n",
    "        for tta in ttas:\n",
    "            tta_x = tta.iloc[trn_]\n",
    "            trn_x = pd.concat((trn_x, tta_x), axis=0)\n",
    "            tta_y = y.iloc[trn_]\n",
    "            trn_y = pd.concat((trn_y, tta_y), axis=0)\n",
    "            tta_w = w.iloc[trn_]\n",
    "            trn_w = pd.concat((trn_w, tta_w), axis=0)\n",
    "            val_x = pd.concat((val_x, tta.iloc[val_]), axis=0)\n",
    "            val_y = pd.concat((val_y, y.iloc[val_]), axis=0)\n",
    "            val_w = pd.concat((val_w, w.iloc[val_]), axis=0)\n",
    "        trn_x = lgb.Dataset(trn_x, label=trn_y, weight=trn_w)\n",
    "        val_x = lgb.Dataset(val_x, label=val_y, weight=val_w)\n",
    "        clf = lgb.train(\n",
    "            lgb_params,\n",
    "            trn_x, \n",
    "            num_boost_round = 4000,\n",
    "            valid_sets=[trn_x, val_x],\n",
    "            valid_names = ['train', 'val'],\n",
    "            feval=eval_lgb_multi_weighted_logloss,\n",
    "            verbose_eval=verbose,\n",
    "            early_stopping_rounds=100\n",
    "        )\n",
    "        val_x = full_train.iloc[val_]\n",
    "        \n",
    "        oof_pred = clf.predict(val_x)\n",
    "        oof_preds[val_, :] = oof_pred\n",
    "        tta_pred = np.zeros(oof_pred.shape)\n",
    "        for tta in ttas:\n",
    "            val_x = tta.iloc[val_]\n",
    "            tta_pred += clf.predict(val_x)\n",
    "        tta_pred /= len(ttas)\n",
    "        tta_preds[val_, :] = tta_pred\n",
    "        print('val mwloss: %0.3f' % multi_weighted_logloss( y.iloc[val_], oof_pred),\n",
    "              'tta mwloss: %0.3f' % multi_weighted_logloss( y.iloc[val_], tta_pred),\n",
    "             )\n",
    "        \n",
    "        clfs.append(clf)\n",
    "\n",
    "    get_logger().info('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "\n",
    "    importances = get_importances(clfs)\n",
    "    return clfs, importances, oof_preds, tta_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(full_train, train_bazin, newling):\n",
    "    # get the right object_ids for the result.\n",
    "    full_train = full_train.merge(train_bazin, how='left', on='object_id')\n",
    "    full_train = full_train.merge(newling, how='left', on='object_id')\n",
    "    for pb in range(6):\n",
    "        full_train['bazin_A_%d' % pb] *= full_train.hostgal_photoz **2 * full_train.scale_mean\n",
    "        full_train['newling_A_%d' % pb] *= full_train.hostgal_photoz **2 * full_train.scale_mean\n",
    "    full_train['bazin_magnitude'] = full_train[['bazin_A_%d' % pb  for pb in range(6)]].max(axis=1)\n",
    "    full_train['newling_magnitude'] = full_train[['newling_A_%d' % pb  for pb in range(6)]].max(axis=1)\n",
    "    for pb in range(6):\n",
    "        full_train['bazin_A_%d' % pb] /= full_train['bazin_magnitude']\n",
    "        full_train['newling_A_%d' % pb] /= full_train['newling_magnitude']\n",
    "    full_train.hostgal_photoz = 1*(full_train.hostgal_photoz > 0)\n",
    "    return full_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/ttas_%s.pkl' % fname_base, 'rb') as file:\n",
    "    ttas = pkl.load(file)\n",
    "full_train = ttas[0]\n",
    "ttas = ttas[1 : n_tta]\n",
    "full_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = ['object_id', 'hostgal_photoz', 'mwebv', 'target']\n",
    "meta_train = pd.read_csv('../data/train_meta.csv')[meta_cols]\n",
    "meta_cols = ['object_id', 'hostgal_photoz', 'mwebv']\n",
    "meta_test = pd.read_csv('../input/test_set_metadata.csv')[meta_cols]\n",
    "\n",
    "meta_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tta_0_%s.pkl' % fname_bazin, 'rb') as file:\n",
    "    train_bazin = pkl.load(file)\n",
    "train_bazin.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttas_bazin = []\n",
    "for i in range(1, n_tta):\n",
    "    with open('../data/tta_%d_%s.pkl' % (i, fname_bazin), 'rb') as file:\n",
    "        ttas_bazin.append(pkl.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttas_bazin[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tta_0_%s.pkl' % fname_newling, 'rb') as file:\n",
    "    train_newling = pkl.load(file)\n",
    "train_newling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttas_newling = []\n",
    "for i in range(1, n_tta):\n",
    "    with open('../data/tta_%d_%s.pkl' % (i, fname_newling ), 'rb') as file:\n",
    "        ttas_newling.append(pkl.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = get_data(full_train, train_bazin, train_newling)\n",
    "full_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tta = 6\n",
    "init_seeds(seed)\n",
    "\n",
    "ttas = [get_data(full_train, train_bazin, train_newling) \\\n",
    "        for full_train, train_bazin, train_newling \\\n",
    "        in tqdm_notebook(zip(ttas, ttas_bazin, ttas_newling))]\n",
    "#for tta in ttas:\n",
    "#    tta.fillna(train_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(np.unique(meta_train.target))\n",
    "classes\n",
    "\n",
    "class_names = ['class_%d' % c for c in classes]\n",
    "\n",
    "weights = [1/18  if i not in [15, 64, 99] else 1/9 for i in classes]\n",
    "weights\n",
    "\n",
    "df = meta_train.groupby('target').object_id.count().to_frame('freq')\n",
    "df.freq /= df.freq.sum()\n",
    "df['weight'] = weights\n",
    "df['adjust'] = df.weight / df.freq\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = meta_train['target']\n",
    "\n",
    "ws = y.copy()\n",
    "for c,w in zip(classes, df.adjust.values):\n",
    "    print(c, w)\n",
    "    ws[y == c] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lgb = y.copy()\n",
    "for i,c in enumerate(classes):\n",
    "    y_lgb[y_lgb == c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(full_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = [\n",
    "    'object_id',\n",
    "    'target',\n",
    "    \n",
    "    'newling_A_1',\n",
    "    'newling_A_2',\n",
    "    'newling_A_3',\n",
    "    'newling_A_4',\n",
    "    'newling_A_5',\n",
    "    'newling_k_1',\n",
    "    'newling_k_2',\n",
    "    'newling_k_3',\n",
    "    'newling_k_4',\n",
    "    'newling_k_5',\n",
    "    \n",
    "    'bazin_pcov_0',\n",
    "    'bazin_pcov_1',\n",
    "    'bazin_pcov_2',\n",
    "    'bazin_pcov_3',\n",
    "    'bazin_pcov_4',\n",
    "    'bazin_pcov_5',\n",
    "    'bazin_max_0',\n",
    "    'bazin_max_1',\n",
    "    'bazin_max_2',\n",
    "    'bazin_max_3',\n",
    "    'bazin_max_4',\n",
    "    'bazin_max_5',\n",
    "    #'bazin_A_0',\n",
    "    #'bazin_A_1',\n",
    "    #'bazin_A_2',\n",
    "    #'bazin_A_3',\n",
    "    #'bazin_A_4',\n",
    "    #'bazin_A_5',\n",
    "    'bazin_before_0',\n",
    "    'bazin_before_1',\n",
    "    'bazin_before_2',\n",
    "    'bazin_before_3',\n",
    "    'bazin_before_4',\n",
    "    'bazin_before_5',\n",
    "    'bazin_after_0',\n",
    "    'bazin_after_1',\n",
    "    'bazin_after_2',\n",
    "    'bazin_after_3',\n",
    "    'bazin_after_4',\n",
    "    'bazin_after_5',\n",
    "    \n",
    "    #'bazin_trise',\n",
    "    \n",
    "    'mwebv',\n",
    "    'num_obs',\n",
    "]\n",
    "\n",
    "features = [c for c in full_train.columns if c not in removed]\n",
    "\n",
    "features\n",
    "\n",
    "full_train1 = full_train[features].copy()\n",
    "ttas1 = [tta[features].copy() for tta in ttas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'goss',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 14,\n",
    "    'metric': 'None',\n",
    "    'learning_rate': 0.015,\n",
    "    'colsample_bytree': .5,\n",
    "    'feature_fraction_seed':seed+2,\n",
    "    'reg_alpha': .01,\n",
    "    'reg_lambda': .1,\n",
    "    'min_split_gain': 0.1,\n",
    "    'min_child_weight': 20 * (1 + len(ttas)),\n",
    "    #'n_estimators': 4000,\n",
    "    #'silent': -1,\n",
    "    'verbose': -1,\n",
    "    #'max_depth': 4,\n",
    "    'num_leaves' : 7,\n",
    "    #'num_threads': 10,\n",
    "}\n",
    "clfs, importances, oof_preds, tta_preds = train_classifiers(lgb_params, full_train1, y_lgb, ws, \n",
    "                                                            folds=10, ttas=ttas1)\n",
    "\n",
    "#save_importances(importances_=importances)\n",
    "print('%0.5f' % multi_weighted_logloss(y, oof_preds), \n",
    "      '%0.5f' % multi_weighted_logloss(y, tta_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/oof_preds_%s.pkl' % fname, 'wb') as file:\n",
    "    pkl.dump(oof_preds, file)\n",
    "    \n",
    "with open('../data/tta_preds_%s.pkl' % fname, 'wb') as file:\n",
    "    pkl.dump(tta_preds, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chunk(clfs_, features, class_names, chunk_id, fname_base, \n",
    "                  fname_bazin=fname_bazin, fname_newling=fname_newling, \n",
    "                  ):\n",
    "\n",
    "    with open('../data/full_test_chunk_%s_%d.pkl' % (fname_base, chunk_id), 'rb') as file:\n",
    "        full_test = pkl.load(file)\n",
    "        \n",
    "    with open('../data/bazin_test_%d_%s.pkl' % (chunk_id, fname_bazin), 'rb') as file:\n",
    "        test_bazin = pkl.load(file)\n",
    "    \n",
    "    with open('../data/test_%d_%s.pkl' % (chunk_id, fname_newling), 'rb') as file:\n",
    "            test_newling = pkl.load(file)  \n",
    "            \n",
    "    if ('newling_sigma_1') not in test_newling.columns:\n",
    "        test_newling['newling_sigma_1'] = np.NaN\n",
    "            \n",
    "    full_test = get_data(full_test, test_bazin, test_newling)\n",
    "    #full_test = full_test.fillna(train_mean)\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict(full_test[features]) / len(clfs_)\n",
    "        else:\n",
    "            preds_ += clf.predict(full_test[features]) / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "    \n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=class_names)\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = preds_99\n",
    "\n",
    "    print(preds_df_['class_99'].mean())\n",
    "\n",
    "    del full_test, preds_\n",
    "    gc.collect()\n",
    "\n",
    "    return preds_df_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "chunks = 5000000\n",
    "remain_df = None\n",
    "\n",
    "for i_c in tqdm_notebook(range(91)):\n",
    "\n",
    "    preds_df = predict_chunk(clfs_=clfs,\n",
    "                             features=features,\n",
    "                             class_names=class_names,\n",
    "                             chunk_id=i_c,\n",
    "                             fname_base=fname_base,\n",
    "                            )\n",
    "\n",
    "    if i_c == 0:\n",
    "        print(preds_df.mean(axis=0))\n",
    "        preds_df.to_csv('../submissions/%s.csv' %fname, header=True, index=False, float_format='%.6f')\n",
    "    else:\n",
    "        preds_df.to_csv('../submissions/%s.csv' %fname, header=False, mode='a', index=False, float_format='%.6f')\n",
    "\n",
    "    del preds_df\n",
    "    gc.collect()\n",
    "\n",
    "    if (i_c + 1) % 10 == 0:\n",
    "        get_logger().info('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "        print('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "\n",
    "# Compute last object in remain_df\n",
    "\n",
    "preds_df = predict_chunk(clfs_=clfs,\n",
    "                         rnn_test=rnn_test,\n",
    "                         features=features,\n",
    "                         class_names=class_names,\n",
    "                         chunk_id=100,\n",
    "                         fname_base=fname_base,\n",
    "                        )\n",
    "\n",
    "preds_df.to_csv('../submissions/%s.csv' %fname, \n",
    "                header=False, mode='a', index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.read_csv('../submissions/%s.csv' %fname)\n",
    "\n",
    "z = z.groupby('object_id').mean()\n",
    "\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = ['hostgal_photoz', 'target']\n",
    "meta_train2 = pd.read_csv('../input/training_set_metadata.csv')[meta_cols]\n",
    "meta_train2.head()\n",
    "\n",
    "df = meta_train2.groupby('target').hostgal_photoz.mean()\n",
    "\n",
    "galactic = ['class_%d' % c for c in df[df == 0].index]\n",
    "extragal = ['class_%d' % c for c in df[df > 0].index]\n",
    "galactic, extragal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z.reset_index()\n",
    "\n",
    "z['class_99'] *= (0.18 / z['class_99'].mean())\n",
    "\n",
    "z.loc[meta_test.hostgal_photoz == 0, extragal] = 0\n",
    "\n",
    "z.loc[meta_test.hostgal_photoz > 0, galactic] = 0\n",
    "\n",
    "z.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z.to_csv('../submissions/gal_%s.csv' %fname, index=False, float_format='%.6f')\n",
    "\n",
    "z['class_99'] = (1. - z[z.columns[1:-1]]).prod(axis=1)\n",
    "\n",
    "z.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z['class_99'] *= (0.18 / z['class_99'].mean())\n",
    "\n",
    "z.to_csv('../submissions/gal_2_%s.csv' %fname, index=False, float_format='%.6f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xgb8]",
   "language": "python",
   "name": "conda-env-xgb8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
